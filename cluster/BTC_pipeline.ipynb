{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ffa782",
   "metadata": {},
   "source": [
    "# BTC Change Detection Inference Pipeline\n",
    "\n",
    "This notebook demonstrates how to perform change detection inference using the BTC (Be The Change) model. \n",
    "\n",
    "The pipeline includes:\n",
    "1. Loading and converting TIFF images to PNG\n",
    "2. Preprocessing images with normalization\n",
    "3. Loading the pre-trained BTC model checkpoint\n",
    "4. Running inference to detect changes\n",
    "5. Visualizing results at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6242ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gasper/Documents/ViCos/eo_cd_slo/cluster/btc_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Add the BTC directory to Python path to import BTC modules\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m btc_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBTC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(btc_path))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Import BTC-specific modules\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Add the BTC directory to Python path to import BTC modules\n",
    "btc_path = Path(__file__).parent.parent / \"BTC\"\n",
    "sys.path.append(str(btc_path))\n",
    "\n",
    "# Import BTC-specific modules\n",
    "from models.finetune_framework import FinetuneFramework\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryF1Score,\n",
    "    BinaryRecall,\n",
    "    BinaryPrecision,\n",
    "    BinaryJaccardIndex,\n",
    ")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b883826",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the paths and parameters for the inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496efa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters (from BTC-B.yaml)\n",
    "CONFIG = {\n",
    "    'img_size': 256,\n",
    "    'normalize_mean': [0.485, 0.456, 0.406],  # ImageNet mean\n",
    "    'normalize_std': [0.229, 0.224, 0.225],   # ImageNet std\n",
    "    'model_checkpoint': 'blaz-r/BTC-B_oscd96',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Input image paths - MODIFY THESE PATHS TO YOUR IMAGES\n",
    "image_a_path = \"path/to/your/image_a.tiff\"  # Before image\n",
    "image_b_path = \"path/to/your/image_b.tiff\"  # After image\n",
    "\n",
    "# Output directory for converted PNG images\n",
    "output_dir = Path(\"./converted_images\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(f\"Device: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f16c9",
   "metadata": {},
   "source": [
    "## Step 1: Convert TIFF Images to PNG\n",
    "\n",
    "First, we'll load the TIFF images and convert them to PNG format for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tiff_to_png(tiff_path, output_dir, filename):\n",
    "    \"\"\"\n",
    "    Convert TIFF image to PNG format and resize to 256x256 if needed.\n",
    "    \n",
    "    Args:\n",
    "        tiff_path: Path to input TIFF file\n",
    "        output_dir: Directory to save PNG file\n",
    "        filename: Output filename (without extension)\n",
    "    \n",
    "    Returns:\n",
    "        Path to converted PNG file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load TIFF image\n",
    "        img = Image.open(tiff_path)\n",
    "        print(f\"Original image size: {img.size}\")\n",
    "        print(f\"Original image mode: {img.mode}\")\n",
    "        \n",
    "        # Convert to RGB if necessary\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "            print(f\"Converted to RGB mode\")\n",
    "        \n",
    "        # Resize to 256x256 if not already\n",
    "        if img.size != (CONFIG['img_size'], CONFIG['img_size']):\n",
    "            img = img.resize((CONFIG['img_size'], CONFIG['img_size']), Image.LANCZOS)\n",
    "            print(f\"Resized to {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "        \n",
    "        # Save as PNG\n",
    "        png_path = output_dir / f\"{filename}.png\"\n",
    "        img.save(png_path, 'PNG')\n",
    "        print(f\"Saved PNG to: {png_path}\")\n",
    "        \n",
    "        return png_path, np.array(img)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {tiff_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Convert both images\n",
    "print(\"Converting Image A (before):\")\n",
    "png_a_path, img_a_array = convert_tiff_to_png(image_a_path, output_dir, \"image_a\")\n",
    "\n",
    "print(\"\\nConverting Image B (after):\")\n",
    "png_b_path, img_b_array = convert_tiff_to_png(image_b_path, output_dir, \"image_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the converted PNG images\n",
    "if img_a_array is not None and img_b_array is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].imshow(img_a_array)\n",
    "    axes[0].set_title('Image A (Before) - PNG')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img_b_array)\n",
    "    axes[1].set_title('Image B (After) - PNG')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Image A shape: {img_a_array.shape}\")\n",
    "    print(f\"Image B shape: {img_b_array.shape}\")\n",
    "    print(f\"Image A pixel range: [{img_a_array.min()}, {img_a_array.max()}]\")\n",
    "    print(f\"Image B pixel range: [{img_b_array.min()}, {img_b_array.max()}]\")\n",
    "else:\n",
    "    print(\"Error: Could not load images. Please check the file paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915e12e",
   "metadata": {},
   "source": [
    "## Step 2: Image Preprocessing and Normalization\n",
    "\n",
    "Now we'll apply the same preprocessing pipeline used by the BTC model:\n",
    "1. Normalize pixel values to [0, 1] range\n",
    "2. Apply ImageNet normalization (subtract mean, divide by std)\n",
    "3. Convert to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_array, config):\n",
    "    \"\"\"\n",
    "    Preprocess image according to BTC model requirements.\n",
    "    \n",
    "    Args:\n",
    "        image_array: numpy array of shape (H, W, 3) with values in [0, 255]\n",
    "        config: configuration dictionary with normalization parameters\n",
    "    \n",
    "    Returns:\n",
    "        preprocessed tensor of shape (1, 3, H, W)\n",
    "    \"\"\"\n",
    "    # Step 1: Convert to float and normalize to [0, 1]\n",
    "    image_float = image_array.astype(np.float32) / 255.0\n",
    "    print(f\"After [0,1] normalization - range: [{image_float.min():.3f}, {image_float.max():.3f}]\")\n",
    "    \n",
    "    # Step 2: Apply Albumentations transforms (same as BTC training)\n",
    "    transform = A.Compose([\n",
    "        A.Normalize(\n",
    "            mean=config['normalize_mean'], \n",
    "            std=config['normalize_std']\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    # Apply transforms\n",
    "    transformed = transform(image=image_float)\n",
    "    tensor = transformed['image']\n",
    "    \n",
    "    print(f\"After ImageNet normalization - range: [{tensor.min():.3f}, {tensor.max():.3f}]\")\n",
    "    print(f\"Tensor shape: {tensor.shape}\")\n",
    "    \n",
    "    # Add batch dimension\n",
    "    tensor = tensor.unsqueeze(0)  # Shape: (1, 3, H, W)\n",
    "    \n",
    "    return tensor, image_float\n",
    "\n",
    "# Preprocess both images\n",
    "print(\"Preprocessing Image A:\")\n",
    "tensor_a, normalized_a = preprocess_image(img_a_array, CONFIG)\n",
    "\n",
    "print(\"\\nPreprocessing Image B:\")\n",
    "tensor_b, normalized_b = preprocess_image(img_b_array, CONFIG)\n",
    "\n",
    "print(f\"\\nFinal tensor shapes:\")\n",
    "print(f\"Tensor A: {tensor_a.shape}\")\n",
    "print(f\"Tensor B: {tensor_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d959881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preprocessing steps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original images\n",
    "axes[0, 0].imshow(img_a_array)\n",
    "axes[0, 0].set_title('Original Image A')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(img_b_array)\n",
    "axes[1, 0].set_title('Original Image B')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Normalized to [0,1]\n",
    "axes[0, 1].imshow(normalized_a)\n",
    "axes[0, 1].set_title('Image A - Normalized [0,1]')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(normalized_b)\n",
    "axes[1, 1].set_title('Image B - Normalized [0,1]')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# After ImageNet normalization (denormalized for visualization)\n",
    "def denormalize_for_viz(tensor, mean, std):\n",
    "    \"\"\"Denormalize tensor for visualization\"\"\"\n",
    "    tensor_copy = tensor.clone()\n",
    "    for i, (m, s) in enumerate(zip(mean, std)):\n",
    "        tensor_copy[i] = tensor_copy[i] * s + m\n",
    "    return torch.clamp(tensor_copy, 0, 1)\n",
    "\n",
    "viz_a = denormalize_for_viz(tensor_a[0], CONFIG['normalize_mean'], CONFIG['normalize_std'])\n",
    "viz_b = denormalize_for_viz(tensor_b[0], CONFIG['normalize_mean'], CONFIG['normalize_std'])\n",
    "\n",
    "axes[0, 2].imshow(viz_a.permute(1, 2, 0))\n",
    "axes[0, 2].set_title('Image A - After ImageNet Norm\\n(denormalized for viz)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(viz_b.permute(1, 2, 0))\n",
    "axes[1, 2].set_title('Image B - After ImageNet Norm\\n(denormalized for viz)')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e6059",
   "metadata": {},
   "source": [
    "## Step 3: Load Pre-trained BTC Model\n",
    "\n",
    "Download and load the pre-trained BTC-B model checkpoint from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051925c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BTC model\n",
    "print(f\"Loading model checkpoint: {CONFIG['model_checkpoint']}\")\n",
    "print(\"This may take a few minutes for the first time...\")\n",
    "\n",
    "try:\n",
    "    # Create metrics collection (required for model loading)\n",
    "    metrics = MetricCollection({\n",
    "        \"F1\": BinaryF1Score(),\n",
    "        \"Recall\": BinaryRecall(),\n",
    "        \"Precision\": BinaryPrecision(),\n",
    "        \"cIoU\": BinaryJaccardIndex(),\n",
    "    })\n",
    "    \n",
    "    # Load the model from HuggingFace\n",
    "    model = FinetuneFramework.from_pretrained(\n",
    "        CONFIG['model_checkpoint'],\n",
    "        metrics=metrics,\n",
    "        logger=None\n",
    "    )\n",
    "    \n",
    "    # Move model to appropriate device\n",
    "    model = model.to(CONFIG['device'])\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully!\")\n",
    "    print(f\"✓ Model moved to device: {CONFIG['device']}\")\n",
    "    print(f\"✓ Model set to evaluation mode\")\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Please check your internet connection and model checkpoint name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edd1d0",
   "metadata": {},
   "source": [
    "## Step 4: Run Change Detection Inference\n",
    "\n",
    "Perform inference using the loaded model to detect changes between the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input batch for the model\n",
    "def prepare_batch(tensor_a, tensor_b, device):\n",
    "    \"\"\"\n",
    "    Prepare input batch for BTC model inference.\n",
    "    \n",
    "    Args:\n",
    "        tensor_a: preprocessed tensor for image A\n",
    "        tensor_b: preprocessed tensor for image B\n",
    "        device: target device\n",
    "    \n",
    "    Returns:\n",
    "        batch dictionary ready for model input\n",
    "    \"\"\"\n",
    "    batch = {\n",
    "        'imageA': tensor_a.to(device),\n",
    "        'imageB': tensor_b.to(device)\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "# Run inference\n",
    "print(\"Running change detection inference...\")\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        # Prepare input batch\n",
    "        batch = prepare_batch(tensor_a, tensor_b, CONFIG['device'])\n",
    "        \n",
    "        print(f\"Input shapes:\")\n",
    "        print(f\"  Image A: {batch['imageA'].shape}\")\n",
    "        print(f\"  Image B: {batch['imageB'].shape}\")\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            start_time.record()\n",
    "        \n",
    "        # Model inference\n",
    "        output = model(batch)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            end_time.record()\n",
    "            torch.cuda.synchronize()\n",
    "            inference_time = start_time.elapsed_time(end_time)\n",
    "            print(f\"Inference time: {inference_time:.2f} ms\")\n",
    "        \n",
    "        print(f\"✓ Inference completed successfully!\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        print(f\"Output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        probabilities = torch.sigmoid(output)\n",
    "        print(f\"Probability range: [{probabilities.min():.4f}, {probabilities.max():.4f}]\")\n",
    "        \n",
    "        # Create binary mask (threshold at 0.5)\n",
    "        binary_mask = (probabilities > 0.5).float()\n",
    "        \n",
    "        # Move results to CPU for visualization\n",
    "        output_cpu = output.cpu().squeeze()\n",
    "        prob_cpu = probabilities.cpu().squeeze()\n",
    "        mask_cpu = binary_mask.cpu().squeeze()\n",
    "        \n",
    "        print(f\"Results moved to CPU for visualization\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during inference: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34bd23",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Results\n",
    "\n",
    "Display the inference results including the original images, probability map, and binary change mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26763f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Row 1: Original images and probability map\n",
    "axes[0, 0].imshow(img_a_array)\n",
    "axes[0, 0].set_title('Image A (Before)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(img_b_array)\n",
    "axes[0, 1].set_title('Image B (After)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Probability map (heatmap)\n",
    "prob_map = axes[0, 2].imshow(prob_cpu.numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title('Change Probability Map', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "plt.colorbar(prob_map, ax=axes[0, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Row 2: Binary mask, overlay, and statistics\n",
    "axes[1, 0].imshow(mask_cpu.numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "axes[1, 0].set_title('Binary Change Mask\\n(Threshold = 0.5)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Overlay change mask on Image B\n",
    "overlay = img_b_array.copy()\n",
    "change_pixels = mask_cpu.numpy() > 0.5\n",
    "overlay[change_pixels] = [255, 0, 0]  # Red for changes\n",
    "axes[1, 1].imshow(overlay)\n",
    "axes[1, 1].set_title('Changes Overlaid on Image B\\n(Red = Change)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Statistics and summary\n",
    "total_pixels = mask_cpu.numel()\n",
    "changed_pixels = torch.sum(mask_cpu).item()\n",
    "change_percentage = (changed_pixels / total_pixels) * 100\n",
    "\n",
    "stats_text = f\"\"\"\n",
    "Change Detection Results:\n",
    "\n",
    "Total pixels: {total_pixels:,}\n",
    "Changed pixels: {changed_pixels:,}\n",
    "Change percentage: {change_percentage:.2f}%\n",
    "\n",
    "Model output range: \n",
    "  Min: {output_cpu.min():.4f}\n",
    "  Max: {output_cpu.max():.4f}\n",
    "\n",
    "Probability range:\n",
    "  Min: {prob_cpu.min():.4f}\n",
    "  Max: {prob_cpu.max():.4f}\n",
    "\n",
    "Mean probability: {prob_cpu.mean():.4f}\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes, \n",
    "                fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "axes[1, 2].set_xlim(0, 1)\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].axis('off')\n",
    "axes[1, 2].set_title('Summary Statistics', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHANGE DETECTION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Images processed: {CONFIG['img_size']}x{CONFIG['img_size']} pixels\")\n",
    "print(f\"✓ Model used: {CONFIG['model_checkpoint']}\")\n",
    "print(f\"✓ Device: {CONFIG['device']}\")\n",
    "print(f\"✓ Changed pixels detected: {changed_pixels:,} ({change_percentage:.2f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092c349",
   "metadata": {},
   "source": [
    "## Optional: Save Results\n",
    "\n",
    "Save the generated masks and visualizations to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to disk\n",
    "save_results = True  # Set to False if you don't want to save\n",
    "\n",
    "if save_results:\n",
    "    results_dir = Path(\"./results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Save probability map as grayscale image\n",
    "        prob_img = Image.fromarray((prob_cpu.numpy() * 255).astype(np.uint8))\n",
    "        prob_path = results_dir / \"probability_map.png\"\n",
    "        prob_img.save(prob_path)\n",
    "        print(f\"✓ Probability map saved to: {prob_path}\")\n",
    "        \n",
    "        # Save binary mask\n",
    "        mask_img = Image.fromarray((mask_cpu.numpy() * 255).astype(np.uint8))\n",
    "        mask_path = results_dir / \"binary_mask.png\"\n",
    "        mask_img.save(mask_path)\n",
    "        print(f\"✓ Binary mask saved to: {mask_path}\")\n",
    "        \n",
    "        # Save overlay image\n",
    "        overlay_img = Image.fromarray(overlay.astype(np.uint8))\n",
    "        overlay_path = results_dir / \"overlay_result.png\"\n",
    "        overlay_img.save(overlay_path)\n",
    "        print(f\"✓ Overlay result saved to: {overlay_path}\")\n",
    "        \n",
    "        # Save raw model output as numpy array\n",
    "        np.save(results_dir / \"raw_output.npy\", output_cpu.numpy())\n",
    "        print(f\"✓ Raw model output saved to: {results_dir / 'raw_output.npy'}\")\n",
    "        \n",
    "        # Save statistics to text file\n",
    "        with open(results_dir / \"statistics.txt\", \"w\") as f:\n",
    "            f.write(f\"Change Detection Results\\n\")\n",
    "            f.write(f\"{'='*30}\\n\")\n",
    "            f.write(f\"Model: {CONFIG['model_checkpoint']}\\n\")\n",
    "            f.write(f\"Image size: {CONFIG['img_size']}x{CONFIG['img_size']}\\n\")\n",
    "            f.write(f\"Total pixels: {total_pixels:,}\\n\")\n",
    "            f.write(f\"Changed pixels: {changed_pixels:,}\\n\")\n",
    "            f.write(f\"Change percentage: {change_percentage:.2f}%\\n\")\n",
    "            f.write(f\"Model output range: [{output_cpu.min():.4f}, {output_cpu.max():.4f}]\\n\")\n",
    "            f.write(f\"Probability range: [{prob_cpu.min():.4f}, {prob_cpu.max():.4f}]\\n\")\n",
    "            f.write(f\"Mean probability: {prob_cpu.mean():.4f}\\n\")\n",
    "        \n",
    "        print(f\"✓ Statistics saved to: {results_dir / 'statistics.txt'}\")\n",
    "        print(f\"\\nAll results saved in: {results_dir.absolute()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving results: {e}\")\n",
    "else:\n",
    "    print(\"Skipping save results (save_results = False)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
